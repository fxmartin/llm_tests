# Comparative Analysis of Local LLM Performance (vs. ChatGPT)
*This report has been generated by ChatGPT based on test results collected from 17 locally run Large Language Models (LLMs) using Ollama. The test involved executing a document summarization task across all models and capturing key performance metrics, including response duration, token usage, and response quality. The collected results were then analyzed and compared against ChatGPT’s standard in terms of efficiency, correctness, coherence, and completeness. The analysis provides a structured evaluation of each model’s performance, highlighting the best-performing models, areas for improvement, and strategic recommendations for selecting the most suitable LLMs based on speed, token efficiency, and summary clarity.*

## 1. Executive Summary

This report evaluates the performance of 17 locally run large language models (LLMs) in summarizing a business document and compares their responses against ChatGPT. The analysis is based on **response duration, token efficiency, and overall quality** relative to ChatGPT. Key findings:

- **Top Performers (Efficiency + Quality)**:  
  - **Dolphin3:latest** (Fastest: 11.21s, 304 tokens)
  - **qwen2.5-coder:latest** (10.65s, 325 tokens)
  - **llama3.1:8b** (10.69s, 315 tokens)
  - **gemma2:latest** (13.32s, 346 tokens)
  - **llama3:latest** (13.48s, 496 tokens)

- **Models with High Quality but Lower Efficiency**:  
  - **deepseek-r1:32b** (85.69s, 985 tokens)  
  - **openthinker:32b** (91.5s, 1000 tokens)  

- **Underperformers** (Slow response time or excessive token use):  
  - **openthinker:32b** (91.5s, 1000 tokens)  
  - **deepseek-r1:32b** (85.69s, 985 tokens)  
  - **deepseek-r1:14b** (41.0s, 911 tokens)  

### Key Trends:
- **Speed vs. Token Usage**: The fastest models used significantly fewer tokens, but some high-token models provided slightly more detail.
- **Clarity & Conciseness**: Some models, like **Dolphin3** and **qwen2.5-coder**, delivered concise summaries at high speed without losing key points.
- **Performance Gaps**: High-parameter models (e.g., **deepseek-r1:32b, openthinker:32b**) were slow and verbose but did not necessarily offer better summaries than smaller, well-optimized models.

---

## 2. Comparative Analysis

| **LLM Model**           | **Duration (s)** | **Tokens Used** | **Observations** |
|-------------------------|-----------------|----------------|------------------|
| **Dolphin3:latest**     | 11.21           | 304            | Fastest, concise, retains key insights |
| **qwen2.5-coder:latest** | 10.65           | 325            | Highly efficient, professional tone |
| **llama3.1:8b**         | 10.69           | 315            | Compact, high-quality |
| **gemma2:latest**       | 13.32           | 346            | Balanced performance |
| **llama3:latest**       | 13.48           | 496            | Comprehensive yet fast |
| **qwen2.5:14b**         | 22.6            | 409            | Well-structured, mid-range efficiency |
| **deepseek-r1:7b**      | 22.91           | 960            | Uses too many tokens but clear |
| **phi4:14b**            | 25.46           | 470            | Balanced but not the most efficient |
| **llava:latest**        | 15.92           | 661            | Decent quality but not optimal efficiency |
| **mistral-small:24b**   | 39.9            | 471            | Slow response, fair quality |
| **o639/Dolphin3.0-Mistral-24B** | 47.46 | 416 | Clear but response time too high |
| **deepseek-r1:14b**     | 41.0            | 911            | Too slow, too many tokens |
| **codestral:22b**       | 34.83           | 422            | Accurate but could be more efficient |
| **deepscaler:latest**   | 18.25           | 937            | Too verbose, unnecessary verbosity |
| **deepseek-r1:32b**     | 85.69           | 985            | Very slow, heavy token usage |
| **openthinker:32b**     | 91.5            | 1000           | The slowest, too wordy |
| **openthinker:7b**      | 21.47           | 847            | Fair, but high token use |

### Key Takeaways:
1. **Models like Dolphin3, Qwen2.5, and Llama3 performed exceptionally well in both speed and quality.**
2. **Larger models (e.g., deepseek-r1:32b, openthinker:32b) had longer response times but did not necessarily provide better summaries.**
3. **The most efficient models used 300-500 tokens, while verbose models used 900-1000 tokens without adding value.**

---

## 3. Comparison with ChatGPT

Each model’s response was compared against ChatGPT’s standard for:
- **Response Similarity** (Correctness, Coherence, Completeness)
- **Efficiency Difference** (Speed & Token Usage)

| **LLM Model**           | **Similarity to ChatGPT** | **Efficiency Difference** |
|-------------------------|-------------------------|--------------------------|
| **Dolphin3:latest**     | High                     | More efficient |
| **qwen2.5-coder:latest** | High                     | More efficient |
| **llama3.1:8b**         | High                     | More efficient |
| **gemma2:latest**       | High                     | Slightly less efficient |
| **llama3:latest**       | Medium-High              | Balanced |
| **deepseek-r1:7b**      | Medium                   | Less efficient |
| **deepseek-r1:14b**     | Medium                   | Less efficient |
| **deepseek-r1:32b**     | Medium                   | Very inefficient |
| **openthinker:32b**     | Low                      | Extremely inefficient |
| **openthinker:7b**      | Medium                   | Slightly inefficient |
| **codestral:22b**       | High                     | Moderate efficiency |
| **phi4:14b**            | High                     | Balanced |
| **mistral-small:24b**   | Medium                   | Less efficient |
| **o639/Dolphin3.0-Mistral-24B** | Medium | Less efficient |
| **qwen2.5:14b**         | High                     | Efficient |
| **llava:latest**        | Medium                   | Moderate efficiency |
| **deepscaler:latest**   | Medium                   | Less efficient |

### Key Insights:
- **Dolphin3, Qwen2.5, and Llama3 closely matched ChatGPT in quality while being more efficient.**
- **Deepseek-r1:32b and Openthinker:32b failed to justify their slow responses with better quality.**
- **Efficiency differences varied widely, with some models using twice the tokens needed.**

---

## 4. Conclusion & Recommendations

### **Best Models for Speed & Quality**
- **Dolphin3:latest** – Fastest, highly efficient
- **qwen2.5-coder:latest** – Compact, professional output
- **llama3.1:8b** – Best balance of speed and clarity

### **Best Models for Comprehensive Responses**
- **phi4:14b** – Balanced performance
- **llama3:latest** – Slightly more detailed, still fast

### **Worst Performers**
- **deepseek-r1:32b, openthinker:32b** – Too slow and verbose without added quality

### **Final Recommendations**
- **For quick, professional summaries**, use **Dolphin3, Qwen2.5, or Llama3.1**.
- **For slightly more detail but still efficient performance**, use **Phi4 or Llama3**.
- **Avoid using large models like Deepseek 32b and Openthinker 32b unless extra context is required**.

By selecting the most efficient models, users can achieve high-quality results while minimizing response times and token usage.